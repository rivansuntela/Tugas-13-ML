{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxnsvveNYxJJ",
        "outputId": "baed16db-38f3-4b91-e823-54e081e4fcb1"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
            "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
            "\u001b[1;31mOr install 'ipykernel' using the command: '\"c:/Users/Latitude 3490/AppData/Local/Microsoft/WindowsApps/python3.11.exe\" -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "def to_lowercase(text):\n",
        "    return text.lower()\n",
        "\n",
        "# Contoh penggunaan\n",
        "sample_text = \"This is an EXAMPLE Sentence.\"\n",
        "print(to_lowercase(sample_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxy6-x7QdPqH",
        "outputId": "27099c36-c739-4d64-eeb6-a5f7af65ca45"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Contoh penggunaan\n",
        "sample_text = \"Hello, World!\"\n",
        "print(remove_punctuation(sample_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N16eY9z8dRb-",
        "outputId": "3b242490-8581-4a44-8f74-4642254e7a2f"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_numbers(text):\n",
        "    return re.sub(r'\\d+', '', text)\n",
        "\n",
        "# Contoh penggunaan\n",
        "sample_text = \"There are 2 apples and 10 oranges.\"\n",
        "print(remove_numbers(sample_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_cJIwqE-dc1A",
        "outputId": "168f31f0-33c1-42cf-db12-7b77cc32df45"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def tokenize(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Contoh penggunaan\n",
        "sample_text = \"This is an example sentence.\"\n",
        "print(tokenize(sample_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcbsxqv8etEQ",
        "outputId": "611129e3-a893-4899-b064-a7f2ab773fc3"
      },
      "outputs": [],
      "source": [
        "\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "def remove_stopwords(words):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    return [word for word in words if word not in stop_words]\n",
        "\n",
        "# Contoh penggunaan\n",
        "sample_text = \"This is an example showing stopwords removal.\"\n",
        "tokenized_text = (sample_text)\n",
        "\n",
        "print(remove_stopwords(tokenized_text))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uBkU5DCgfOZc",
        "outputId": "3f764288-ebab-4101-c5c6-a40d333e372e"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "def stem_words(words):\n",
        "    ps = PorterStemmer()\n",
        "    return [ps.stem(word) for word in words]\n",
        "\n",
        "# Contoh penggunaan\n",
        "sample_text = \"This is an example showing stemming of words.\"\n",
        "tokenized_text = tokenize(sample_text)\n",
        "filtered_words = remove_stopwords(tokenized_text)\n",
        "print(stem_words(filtered_words))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8n5VQsUgWXp"
      },
      "outputs": [],
      "source": [
        "sentence1 = \"I love football\"\n",
        "sentence2 = \"Messi is a great football player\"\n",
        "sentence3 = \"Messi has won seven Ballon dâ€™Or awards \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcb96JN0goyX",
        "outputId": "456ab1cf-d6af-4bb2-b3df-6e4c2e2c02f1"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "docs = [sentence1, sentence2, sentence3]\n",
        "print(docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "qncwNZRXg1l1",
        "outputId": "b5fd998c-1bbf-4a04-f8a2-2a00480742fa"
      },
      "outputs": [],
      "source": [
        "#Mendefinisikan dan menyesuaikan count vectorizer pada dokumen.\n",
        "\n",
        "vec = CountVectorizer()\n",
        "X = vec.fit_transform(docs)\n",
        "#Mengonversi vektor pada DataFrame menggunakan pandas\n",
        "\n",
        "df = pd.DataFrame(X.toarray(),\n",
        "    columns=vec.get_feature_names_out())\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN06ffX_M49M",
        "outputId": "96c6bc8b-2c75-4f42-e732-aa6204b079fa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "from math import log\n",
        "\n",
        "# Tiga dokumen dalam korpus\n",
        "documents = [\n",
        "    \"The quick brown fox jumps over the lazy dog\",\n",
        "    \"The lazy dog sleeps in the sun\"\n",
        "    ]\n",
        "\n",
        "# Preprocessing: Lowercasing and tokenizing\n",
        "tokenized_documents = [doc.lower().split() for doc in documents]\n",
        "\n",
        "# Menghitung TF\n",
        "def compute_tf(tokenized_doc):\n",
        "    tf_dict = {}\n",
        "    term_count = Counter(tokenized_doc)\n",
        "    total_terms = len(tokenized_doc)\n",
        "    for term, count in term_count.items():\n",
        "        tf_dict[term] = count / total_terms\n",
        "    return tf_dict\n",
        "\n",
        "tf_list = [compute_tf(doc) for doc in tokenized_documents]\n",
        "\n",
        "print(\"Term Frequency (TF):\")\n",
        "for idx, tf in enumerate(tf_list):\n",
        "    print(f\"Document {idx + 1} TF:\")\n",
        "    for term, score in tf.items():\n",
        "        print(f\"    {term}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGpZKYNgO-Ed",
        "outputId": "f2c3ef27-1237-4771-cefc-6e0b84dfd575"
      },
      "outputs": [],
      "source": [
        "# Menghitung IDF\n",
        "def compute_idf(tokenized_docs):\n",
        "    idf_dict = {}\n",
        "    total_docs = len(tokenized_docs)\n",
        "    all_terms = set(term for doc in tokenized_docs for term in doc)\n",
        "    for term in all_terms:\n",
        "        doc_containing_term = sum(1 for doc in tokenized_docs if term in doc)\n",
        "        idf_dict[term] = log(total_docs / (1 + doc_containing_term)) + 1\n",
        "    return idf_dict\n",
        "\n",
        "idf_dict = compute_idf(tokenized_documents)\n",
        "\n",
        "print(\"\\nInverse Document Frequency (IDF):\")\n",
        "for term, score in idf_dict.items():\n",
        "    print(f\"    {term}: {score:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w36ZT0myY6-a",
        "outputId": "6ae62c8d-fc45-4069-85a1-ffb9e33604b7"
      },
      "outputs": [],
      "source": [
        "# Menghitung TF-IDF\n",
        "def compute_tfidf(tf_list, idf_dict):\n",
        "    tfidf_list = []\n",
        "    for tf in tf_list:\n",
        "        tfidf_dict = {}\n",
        "        for term, tf_value in tf.items():\n",
        "            tfidf_dict[term] = tf_value * idf_dict.get(term, 0)\n",
        "        tfidf_list.append(tfidf_dict)\n",
        "    return tfidf_list\n",
        "\n",
        "tfidf_list = compute_tfidf(tf_list, idf_dict)\n",
        "\n",
        "print(\"\\nTF-IDF:\")\n",
        "for idx, tfidf in enumerate(tfidf_list):\n",
        "    print(f\"Document {idx + 1} TF-IDF:\")\n",
        "    for term, score in tfidf.items():\n",
        "        print(f\"    {term}: {score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 993
        },
        "id": "SXXRSD7xITzA",
        "outputId": "2b31eb50-369f-4077-f4f0-d73bfa27da75"
      },
      "outputs": [],
      "source": [
        "pip install --force-reinstall --no-cache-dir gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4KJeGi1jt-k",
        "outputId": "02b2e0e9-de63-410e-e49e-2102d8389ccc"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "\n",
        "corpus = [\n",
        "    'Iam the bones of my sword.',\n",
        "    'Steel is my body and fire is my blood.',\n",
        "    'I created over thousand blade.',\n",
        "    'Unknown to live and unknown to death',\n",
        "    'So as i pray unlimited blade works'\n",
        "]\n",
        "\n",
        "sentences = [doc.split() for doc in corpus]\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def document_vector(doc):\n",
        "    return np.mean([model.wv[word] for word in doc.split() if word in model.wv], axis=0)\n",
        "\n",
        "doc_vectors = [document_vector(doc) for doc in corpus]\n",
        "print(doc_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohgoXHT4v5Nn",
        "outputId": "eaad2b0f-e5f2-415d-cdc1-83cc4b04bd85"
      },
      "outputs": [],
      "source": [
        "!pip install numpy pandas scikit-learn\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1G6TqUQzwYrq"
      },
      "source": [
        "Klasifikasi teks dengan Machine Learning.\n",
        "This dataset is a collection newsgroup documents. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-IzwLWVwAih",
        "outputId": "36c4d6bc-354c-40a7-8693-49b5fd7c3530"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import metrics\n",
        "\n",
        "# 1. Mengumpulkan data\n",
        "newsgroups = fetch_20newsgroups(subset='all')\n",
        "\n",
        "# 2. Preprocessing data\n",
        "# Tidak perlu preprocessing khusus karena kita akan menggunakan TfidfVectorizer\n",
        "\n",
        "# 3. Membagi data menjadi training dan testing set\n",
        "X_train, X_test, y_train, y_test = train_test_split(newsgroups.data, newsgroups.target, test_size=0.25, random_state=42)\n",
        "\n",
        "# 4. Melatih model\n",
        "# Membuat pipeline yang mencakup TfidfVectorizer dan MultinomialNB\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB())\n",
        "\n",
        "# Melatih model menggunakan training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# 5. Mengevaluasi model\n",
        "# Prediksi pada testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluasi kinerja model\n",
        "print(f\"Accuracy: {metrics.accuracy_score(y_test, y_pred)}\")\n",
        "print(\"Classification Report:\")\n",
        "print(metrics.classification_report(y_test, y_pred, target_names=newsgroups.target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(metrics.confusion_matrix(y_test, y_pred))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
